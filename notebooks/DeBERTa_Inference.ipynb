{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "VlLifzPMZVv7",
   "metadata": {
    "id": "VlLifzPMZVv7"
   },
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "import pandas as pd\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "from transformers import DebertaTokenizer, DebertaForSequenceClassification\n",
    "from transformers import T5Tokenizer, T5ForSequenceClassification\n",
    "from tqdm.auto import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f895944-f69d-4b7c-b936-ace27f866bf1",
   "metadata": {
    "id": "9f895944-f69d-4b7c-b936-ace27f866bf1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/jingyuk/elm/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at t5-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight', 'classification_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 32  # Uses ~5GB VRAM\n",
    "\n",
    "# tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n",
    "# ckpt_dir = 'deberta_results/deberta_ckpt/checkpoint-12000'\n",
    "# model = DebertaForSequenceClassification.from_pretrained(ckpt_dir).to(device)\n",
    "ckpt_dir = 'deberta_results/flan_ckpt/checkpoint-4000'\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForSequenceClassification.from_pretrained('t5-base').to(device)\n",
    "\n",
    "def tokenize_function(text):\n",
    "    return tokenizer(text, padding='max_length', return_tensors='pt', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db3ddbd2-0432-4667-b365-d5297c8d75c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_page(html_page: str, objective: str, top_k=(1, 5, 10, 50)):\n",
    "    with torch.no_grad():\n",
    "        elements = [el.strip() for el in html_page.split('\\n')]\n",
    "        elements = [el for el in elements if el]\n",
    "        prompts = [f'Objective: {objective}.\\nElement: {element}' for element in elements]\n",
    "        positive_logits = None\n",
    "    \n",
    "        for j in range(0, len(prompts), batch_size):\n",
    "            ex = tokenize_function(prompts[j:j+batch_size]).to(device)\n",
    "            out = model(**ex)\n",
    "            cur = out.logits[:, 1]\n",
    "            positive_logits = torch.cat((positive_logits, cur)) if positive_logits is not None else cur\n",
    "            del cur, out, ex\n",
    "            gc.collect() \n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "        results = []\n",
    "        for k in top_k:\n",
    "            top_k_indices = sorted(range(len(positive_logits)), key=lambda i: positive_logits[i], reverse=True)[:k]\n",
    "            top_k_labels = [elements[i] for i in top_k_indices]\n",
    "            results.append((k, top_k_labels))\n",
    "    \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DXpMuMApZN3b",
   "metadata": {
    "id": "DXpMuMApZN3b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_file = 'mind2web.txt'\n",
    "file_path = 'data/mind2web.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "val_start_idx =  math.floor(len(df) * 0.8)\n",
    "\n",
    "with open(output_file, 'w') as wf:\n",
    "    for i in tqdm(range(val_start_idx, len(df))):\n",
    "        ex = df.iloc[len(df)-i-1]\n",
    "        action_string = ex['ACTION']\n",
    "        objective = ex['OBJECTIVE']\n",
    "        wf.write(f'{len(df)-i-1}, Task: {objective}; Action: {action_string}\\n')\n",
    "        results = filter_page(ex['OBSERVATION'], objective)\n",
    "        for k, top_k_labels in results:\n",
    "            wf.write(f'k={k}: {top_k_labels}\\n')\n",
    "        wf.write('=' * 50 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3dac34-80b2-4f02-b0b9-9bb2ac5ab715",
   "metadata": {},
   "source": [
    "## Run on WebArena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9c2a187-0766-45bc-8fcc-1697480ed5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/webarena_test.json', 'r') as f:\n",
    "    webarena_data = json.load(f)\n",
    "    id2objective = {}\n",
    "    for d in webarena_data:\n",
    "        id2objective[d['task_id']] = d['intent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1489051-2727-4220-b8b2-be1a17a4dd17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3b8a6acf7842e99b1cefd8665d9600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "base_dir = 'data/webarena_acc_tree'\n",
    "pattern = os.path.join(base_dir, 'render_*_tree_0.txt')\n",
    "output_file = os.path.join(ckpt_dir, 'webarena_results.txt')\n",
    "\n",
    "with open(output_file, 'w') as output:\n",
    "    for file_path in tqdm(glob.glob(pattern)):\n",
    "        with open(file_path, 'r') as f:\n",
    "            html_content = '\\n'.join([s for s in f.readlines()])\n",
    "            # Extracting the task_id from the file name\n",
    "            task_id = int(file_path.split('/')[-1].split('_')[1])\n",
    "            objective = id2objective[task_id]\n",
    "\n",
    "        output.write(f\"{task_id} Task: {objective}\\n\")\n",
    "        results = filter_page(html_content, objective)\n",
    "        for k, top_k_labels in results:\n",
    "            output.write(f\"k={k}: {top_k_labels}\\n\")\n",
    "        output.write('=' * 50 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6ee0fc-40d2-4cce-af38-6ba06247a5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
