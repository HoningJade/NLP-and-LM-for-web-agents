{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f41304d-34f4-4031-a2f0-1e166bd34dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a0f47c6-5f7d-44c3-8d14-8dc4436654bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/jingyuk/elm/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DebertaTokenizer, T5Tokenizer\n",
    "# tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "202c373c-0b54-4f4d-bb60-1e9ab6c6330a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5532 529483 1382 133051\n",
      "Objective: Check for pickup restaurant available in Boston, NY on March 18, 5pm with just one guest.\n",
      "Element: [168] combobox '' hasPopup: menu expanded: False\n",
      "Objective: Check for pickup restaurant available in Boston, NY on March 18, 5pm with just one guest.\n",
      "Element: [1] RootWebArea '' focused: True\n"
     ]
    }
   ],
   "source": [
    "file_path = 'data/mind2web.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "negative_examples = []\n",
    "positive_examples = []\n",
    "negative_examples_id = []\n",
    "positive_examples_id = []\n",
    "val_negative_examples = []\n",
    "val_positive_examples = []\n",
    "val_negative_examples_id = []\n",
    "val_positive_examples_id = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    ex = df.iloc[i]\n",
    "    elements = [el.strip() for el in ex['OBSERVATION'].split('\\n')]\n",
    "    action_string = ex['ACTION']\n",
    "    objective = ex['OBJECTIVE']\n",
    "    \n",
    "    match = re.search(r'\\[(\\d+)\\]', action_string)\n",
    "    groundtruth_id = match.group(1) if match else None\n",
    "    if groundtruth_id:\n",
    "        gt_id_formatted = f'[{groundtruth_id}]'\n",
    "        for x in elements:\n",
    "            if gt_id_formatted in x:\n",
    "                # positive_examples.append((objective, x))\n",
    "                if i < len(df) * 0.8:\n",
    "                    positive_examples.append(f'Objective: {objective}.\\nElement: {x}')\n",
    "                    positive_examples_id.append(i)\n",
    "                else:\n",
    "                    val_positive_examples.append(f'Objective: {objective}.\\nElement: {x}')\n",
    "                    val_positive_examples_id.append(i)\n",
    "            else:\n",
    "                if i < len(df) * 0.8:\n",
    "                    negative_examples.append(f'Objective: {objective}.\\nElement: {x}')\n",
    "                    negative_examples_id.append(i)\n",
    "                else:\n",
    "                    val_negative_examples.append(f'Objective: {objective}.\\nElement: {x}')\n",
    "                    val_negative_examples_id.append(i)\n",
    "\n",
    "print(len(positive_examples), len(negative_examples), len(val_positive_examples), len(val_negative_examples))\n",
    "print(positive_examples[0])\n",
    "print(negative_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c455a0e-7bd2-4e61-a5e0-77b67aa126c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "535015 134433\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame\n",
    "train_df = pd.DataFrame(data = {\n",
    "    'id': positive_examples_id + negative_examples_id,\n",
    "    'text': positive_examples + negative_examples,\n",
    "    'label': [1] * len(positive_examples) + [-1] * len(negative_examples)\n",
    "})\n",
    "val_df = pd.DataFrame(data = {\n",
    "    'id': val_positive_examples_id + val_negative_examples_id,\n",
    "    'text': val_positive_examples + val_negative_examples,\n",
    "    'label': [1] * len(val_positive_examples) + [-1] * len(val_negative_examples)\n",
    "})\n",
    "\n",
    "# TODO(jykoh): Change to 1.0 later\n",
    "train_df = train_df.sample(frac=1.0).reset_index(drop=True)  # Shuffle\n",
    "val_df = val_df#.sample(frac=0.005).reset_index(drop=True)\n",
    "print(len(train_df), len(val_df))\n",
    "\n",
    "train_df.to_csv('train_data.csv', index=False)\n",
    "val_df.to_csv('val_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5b6f703-be70-4661-b069-82e39567497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ids = (val_df['id'].unique())[:100]\n",
    "val_df = val_df[val_df['id'].isin(filtered_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fc492e6-2972-4b90-9529-5c14d88320c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f727175df5b64394abf87029df7c4e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9672 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c98170906e42edbef33b81cf0c94b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/9672 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DebertaTokenizer\n",
    "from datasets import Dataset\n",
    "import random\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Function to tokenize a batch of texts\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# Convert your pandas dataframes to Hugging Face Dataset objects\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# Tokenize the data\n",
    "# tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# positive_dataset = tokenized_train_dataset.filter(lambda example: example['label'] == 1)\n",
    "# negative_dataset = tokenized_train_dataset.filter(lambda example: example['label'] == -1)\n",
    "\n",
    "# # Function to create subsets of the negative dataset\n",
    "# def create_negative_subsets(dataset, subset_size, num_subsets):\n",
    "#     subsets = []\n",
    "#     for _ in range(num_subsets):\n",
    "#         # Randomly sample without replacement\n",
    "#         sampled_indices = random.sample(range(len(dataset)), subset_size)\n",
    "#         subsets.append(dataset.select(sampled_indices))\n",
    "#     return subsets\n",
    "\n",
    "# positive_count, negative_count = len(positive_dataset), len(negative_dataset)\n",
    "# sampling_ratio = negative_count // positive_count\n",
    "\n",
    "# negative_subsets = create_negative_subsets(negative_dataset, positive_count, sampling_ratio)\n",
    "# balanced_datasets = [concatenate_datasets([positive_dataset, neg_subset]).shuffle() for neg_subset in negative_subsets]\n",
    "# combined_balanced_dataset = concatenate_datasets(balanced_datasets).shuffle()\n",
    "\n",
    "# combined_balanced_dataset.save_to_disk('train_t5.hf')\n",
    "tokenized_val_dataset.save_to_disk('val_t5_sampled.hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "407b3cd7-f3ec-454a-8237-b17c46380667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/jingyuk/elm/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "combined_balanced_dataset = load_from_disk('train_t5.hf')\n",
    "tokenized_val_dataset = load_from_disk('val_t5_sampled.hf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0004b0ba-f31a-433b-bab5-2f22e1157686",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadfb95a-cc90-4724-bdec-5d68c6615845",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8df825ea-71d5-4671-aa08-3a5966026df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/jingyuk/elm/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at t5-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.out_proj.bias', 'classification_head.dense.weight', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DebertaTokenizer, DebertaForSequenceClassification\n",
    "from transformers import T5Tokenizer, T5ForSequenceClassification\n",
    "\n",
    "# tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n",
    "# model = DebertaForSequenceClassification.from_pretrained('microsoft/deberta-base', num_labels=2)\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForSequenceClassification.from_pretrained('t5-base', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1857d4f2-1d4d-4dff-a966-134b4dd4a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), ((labels + 1) // 2).view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        eval_dataset = eval_dataset or self.eval_dataset\n",
    "        # Call the original evaluate function\n",
    "        predictions = self.predict(eval_dataset, ignore_keys, metric_key_prefix)\n",
    "        # Convert logits to predicted labels\n",
    "        if type(self.model) == T5ForSequenceClassification:\n",
    "            predicted_labels = np.argmax(predictions.predictions[0], axis=1)\n",
    "        else:\n",
    "            predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Aggregate predictions and true labels by ID\n",
    "        id_recall = {}\n",
    "        bce_loss = []\n",
    "        for idx, id_ in enumerate(eval_dataset['id']):\n",
    "            if id_ not in id_recall:\n",
    "                id_recall[id_] = {'logits': [], 'preds': [], 'labels': []}\n",
    "\n",
    "            if type(self.model) == T5ForSequenceClassification:\n",
    "                id_recall[id_]['logits'].append(predictions.predictions[0][idx, 1])\n",
    "                logits = predictions.predictions[0][idx]\n",
    "            else:\n",
    "                id_recall[id_]['logits'].append(predictions.predictions[idx, 1])\n",
    "                logits = predictions.predictions[idx]\n",
    "            id_recall[id_]['preds'].append(predicted_labels[idx])\n",
    "            id_recall[id_]['labels'].append(predictions.label_ids[idx])\n",
    "\n",
    "            label = predictions.label_ids[idx]\n",
    "            bce_loss.append(loss_fct(torch.tensor([logits]), torch.tensor([int(label == 1)])))\n",
    "\n",
    "        mean_bce_loss = torch.mean(torch.stack(bce_loss))\n",
    "\n",
    "        output = {\n",
    "            'eval_loss': mean_bce_loss.item()\n",
    "        }\n",
    "        # Compute recall for each ID\n",
    "        for k in [1, 5, 10, 50]:\n",
    "            total_recalled = 0\n",
    "            total = 0\n",
    "            total_recalled_random = 0\n",
    "            for id_, data in id_recall.items():\n",
    "                # Sort the logits and get top k indices\n",
    "                top_k_indices = sorted(range(len(data['logits'])), key=lambda i: data['logits'][i], reverse=True)[:k]\n",
    "                # Get the labels corresponding to the top k logits\n",
    "                top_k_labels = [data['labels'][i] for i in top_k_indices]\n",
    "                if 1 in top_k_labels:\n",
    "                    total_recalled += 1\n",
    "                if 1 in [data['labels'][i] for i in random.sample(range(0, len(data['labels'])), min(k, len(data['labels'])))]:\n",
    "                    total_recalled_random += 1\n",
    "                if 1 in data['labels']:\n",
    "                    total += 1\n",
    "            output[f'r@{k}'] = total_recalled / total\n",
    "            output[f'r_rand@{k}'] = total_recalled_random / total\n",
    "\n",
    "        if self.args.logging_dir is not None:\n",
    "            tb_writer = SummaryWriter(log_dir=self.args.logging_dir)\n",
    "            # Log each ID's recall to TensorBoard\n",
    "            print('Results for step', self.state.global_step)\n",
    "            for k, score in output.items():\n",
    "                print(f\"eval/{k}:\", score)\n",
    "                tb_writer.add_scalar(f\"eval/{k}\", score, self.state.global_step)\n",
    "            tb_writer.flush()\n",
    "            tb_writer.close()\n",
    "\n",
    "        # Add aggregated recall scores to output\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad87e510-1bfe-4ed2-807f-caef5aea2e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import datasets\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = logits.argmax(axis=-1)\n",
    "#     print(accuracy_score(labels, predictions))\n",
    "#     return {'accuracy': accuracy_score(labels, predictions)}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./deberta_results',          # directory for storing logs and model checkpoints\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size for training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type= \"cosine\",\n",
    "    weight_decay=0.001,               # strength of weight decay\n",
    "    report_to=\"tensorboard\",\n",
    "    bf16=True,\n",
    "    logging_steps=100,                # log model metrics every 'logging_steps' steps\n",
    "    evaluation_strategy=\"steps\",     # evaluation strategy to adopt during training\n",
    "    eval_steps=2000,                  # number of steps to run evaluation\n",
    "    save_steps=2000,\n",
    "    eval_accumulation_steps=16,\n",
    "    load_best_model_at_end=True      # load the best model when finished training\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=combined_balanced_dataset,  # Use your combined balanced dataset\n",
    "    eval_dataset=tokenized_val_dataset,        # Validation dataset\n",
    "    # compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bf3ad7-5019-4f45-bbb5-2775a99567cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23225' max='197079' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 23225/197079 4:56:54 < 37:02:44, 1.30 it/s, Epoch 0.35/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='605' max='605' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [605/605 03:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3349886/3000023669.py:47: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  bce_loss.append(loss_fct(torch.tensor([logits]), torch.tensor([int(label == 1)])))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for step 2000\n",
      "eval/eval_loss: 0.418666273355484\n",
      "eval/r@1: 0.27\n",
      "eval/r_rand@1: 0.01\n",
      "eval/r@5: 0.67\n",
      "eval/r_rand@5: 0.07\n",
      "eval/r@10: 0.8\n",
      "eval/r_rand@10: 0.06\n",
      "eval/r@50: 0.99\n",
      "eval/r_rand@50: 0.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for step 22000\n",
      "eval/eval_loss: 0.2841876149177551\n",
      "eval/r@1: 0.28\n",
      "eval/r_rand@1: 0.01\n",
      "eval/r@5: 0.68\n",
      "eval/r_rand@5: 0.03\n",
      "eval/r@10: 0.83\n",
      "eval/r_rand@10: 0.1\n",
      "eval/r@50: 0.99\n",
      "eval/r_rand@50: 0.48\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20b1ee6-6fc6-48c1-9cc1-39f2169a5ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
